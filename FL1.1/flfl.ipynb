{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-29 14:28:19.769717: W tensorflow/core/framework/cpu_allocator_impl.cc:80] Allocation of 614400000 exceeds 10% of free system memory.\n",
      "2023-04-29 14:28:19.912862: W tensorflow/core/framework/cpu_allocator_impl.cc:80] Allocation of 614400000 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "begin timestamp : 1682771300.1817064; tag : create_cifar10_model; duration : 0.14760804176330566; package_0 : 781859.0; core_0 : 521789.0; uncore_0 : 72632.0\n",
      "begin timestamp : 1682771300.59314; tag : create_cifar10_model; duration : 0.09064459800720215; package_0 : 579222.0; core_0 : 441833.0; uncore_0 : 33813.0\n",
      "begin timestamp : 1682771301.0125115; tag : create_cifar10_model; duration : 0.08551526069641113; package_0 : 478637.0; core_0 : 349181.0; uncore_0 : 35583.0\n",
      "begin timestamp : 1682771301.5180442; tag : create_cifar10_model; duration : 0.43654870986938477; package_0 : 2471063.0; core_0 : 1687068.0; uncore_0 : 196166.0\n",
      "begin timestamp : 1682771303.6472204; tag : create_cifar10_model; duration : 0.09239006042480469; package_0 : 559997.0; core_0 : 413268.0; uncore_0 : 45227.0\n",
      "WARNING:tensorflow:From /home/gaia/anaconda3/envs/jj1.3/lib/python3.8/site-packages/tensorflow_federated/python/core/impl/compiler/tensorflow_computation_transformations.py:58: extract_sub_graph (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.compat.v1.graph_util.extract_sub_graph`\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/gaia/anaconda3/envs/jj1.3/lib/python3.8/site-packages/tensorflow_federated/python/core/impl/compiler/tensorflow_computation_transformations.py:58: extract_sub_graph (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.compat.v1.graph_util.extract_sub_graph`\n",
      "2023-04-29 14:28:25.740200: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2023-04-29 14:28:25.762628: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2803200000 Hz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Round 1\n",
      "Metrics: OrderedDict([('broadcast', ()), ('aggregation', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('train', OrderedDict([('sparse_categorical_accuracy', 0.13878), ('loss', 2.2802544)])), ('stat', OrderedDict([('num_examples', 50000)]))])\n",
      "Round 2\n",
      "Metrics: OrderedDict([('broadcast', ()), ('aggregation', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('train', OrderedDict([('sparse_categorical_accuracy', 0.209), ('loss', 2.1487389)])), ('stat', OrderedDict([('num_examples', 50000)]))])\n",
      "Round 3\n",
      "Metrics: OrderedDict([('broadcast', ()), ('aggregation', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('train', OrderedDict([('sparse_categorical_accuracy', 0.256), ('loss', 2.0311894)])), ('stat', OrderedDict([('num_examples', 50000)]))])\n",
      "Round 4\n",
      "Metrics: OrderedDict([('broadcast', ()), ('aggregation', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('train', OrderedDict([('sparse_categorical_accuracy', 0.29252), ('loss', 1.945558)])), ('stat', OrderedDict([('num_examples', 50000)]))])\n",
      "Round 5\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 116\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m round_num \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, num_rounds \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRound \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mround_num\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 116\u001b[0m     state, metrics \u001b[38;5;241m=\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnext\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfederated_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    117\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMetrics: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmetrics\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    126\u001b[0m     \u001b[38;5;66;03m# Convert the test dataset into a list of datasets\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/jj1.3/lib/python3.8/site-packages/tensorflow_federated/python/core/impl/computation/function_utils.py:526\u001b[0m, in \u001b[0;36mConcreteFunction.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    524\u001b[0m context \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_context_stack\u001b[39m.\u001b[39mcurrent\n\u001b[1;32m    525\u001b[0m arg \u001b[39m=\u001b[39m pack_args(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_type_signature\u001b[39m.\u001b[39mparameter, args, kwargs, context)\n\u001b[0;32m--> 526\u001b[0m \u001b[39mreturn\u001b[39;00m context\u001b[39m.\u001b[39;49minvoke(\u001b[39mself\u001b[39;49m, arg)\n",
      "File \u001b[0;32m~/anaconda3/envs/jj1.3/lib/python3.8/site-packages/retrying.py:56\u001b[0m, in \u001b[0;36mretry.<locals>.wrap.<locals>.wrapped_f\u001b[0;34m(*args, **kw)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[39m@six\u001b[39m\u001b[39m.\u001b[39mwraps(f)\n\u001b[1;32m     55\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapped_f\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkw):\n\u001b[0;32m---> 56\u001b[0m     \u001b[39mreturn\u001b[39;00m Retrying(\u001b[39m*\u001b[39;49mdargs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mdkw)\u001b[39m.\u001b[39;49mcall(f, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkw)\n",
      "File \u001b[0;32m~/anaconda3/envs/jj1.3/lib/python3.8/site-packages/retrying.py:257\u001b[0m, in \u001b[0;36mRetrying.call\u001b[0;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m    254\u001b[0m     attempt \u001b[39m=\u001b[39m Attempt(tb, attempt_number, \u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m    256\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mshould_reject(attempt):\n\u001b[0;32m--> 257\u001b[0m     \u001b[39mreturn\u001b[39;00m attempt\u001b[39m.\u001b[39;49mget(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_wrap_exception)\n\u001b[1;32m    259\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_after_attempts:\n\u001b[1;32m    260\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_after_attempts(attempt_number)\n",
      "File \u001b[0;32m~/anaconda3/envs/jj1.3/lib/python3.8/site-packages/retrying.py:301\u001b[0m, in \u001b[0;36mAttempt.get\u001b[0;34m(self, wrap_exception)\u001b[0m\n\u001b[1;32m    299\u001b[0m         \u001b[39mraise\u001b[39;00m RetryError(\u001b[39mself\u001b[39m)\n\u001b[1;32m    300\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 301\u001b[0m         six\u001b[39m.\u001b[39;49mreraise(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mvalue[\u001b[39m0\u001b[39;49m], \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mvalue[\u001b[39m1\u001b[39;49m], \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mvalue[\u001b[39m2\u001b[39;49m])\n\u001b[1;32m    302\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    303\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvalue\n",
      "File \u001b[0;32m~/anaconda3/envs/jj1.3/lib/python3.8/site-packages/six.py:719\u001b[0m, in \u001b[0;36mreraise\u001b[0;34m(tp, value, tb)\u001b[0m\n\u001b[1;32m    717\u001b[0m     \u001b[39mif\u001b[39;00m value\u001b[39m.\u001b[39m__traceback__ \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m tb:\n\u001b[1;32m    718\u001b[0m         \u001b[39mraise\u001b[39;00m value\u001b[39m.\u001b[39mwith_traceback(tb)\n\u001b[0;32m--> 719\u001b[0m     \u001b[39mraise\u001b[39;00m value\n\u001b[1;32m    720\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m    721\u001b[0m     value \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/jj1.3/lib/python3.8/site-packages/retrying.py:251\u001b[0m, in \u001b[0;36mRetrying.call\u001b[0;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m    248\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_before_attempts(attempt_number)\n\u001b[1;32m    250\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 251\u001b[0m     attempt \u001b[39m=\u001b[39m Attempt(fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs), attempt_number, \u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m    252\u001b[0m \u001b[39mexcept\u001b[39;00m:\n\u001b[1;32m    253\u001b[0m     tb \u001b[39m=\u001b[39m sys\u001b[39m.\u001b[39mexc_info()\n",
      "File \u001b[0;32m~/anaconda3/envs/jj1.3/lib/python3.8/site-packages/tensorflow_federated/python/core/impl/executors/execution_context.py:215\u001b[0m, in \u001b[0;36mExecutionContext.invoke\u001b[0;34m(self, comp, arg)\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[39mif\u001b[39;00m arg \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    211\u001b[0m   arg \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_event_loop\u001b[39m.\u001b[39mrun_until_complete(\n\u001b[1;32m    212\u001b[0m       tracing\u001b[39m.\u001b[39mwrap_coroutine_in_current_trace_context(\n\u001b[1;32m    213\u001b[0m           _ingest(executor, unwrapped_arg, arg\u001b[39m.\u001b[39mtype_signature)))\n\u001b[0;32m--> 215\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_event_loop\u001b[39m.\u001b[39;49mrun_until_complete(\n\u001b[1;32m    216\u001b[0m     tracing\u001b[39m.\u001b[39;49mwrap_coroutine_in_current_trace_context(\n\u001b[1;32m    217\u001b[0m         _invoke(executor, comp, arg, result_type)))\n",
      "File \u001b[0;32m~/anaconda3/envs/jj1.3/lib/python3.8/site-packages/nest_asyncio.py:84\u001b[0m, in \u001b[0;36m_patch_loop.<locals>.run_until_complete\u001b[0;34m(self, future)\u001b[0m\n\u001b[1;32m     82\u001b[0m     f\u001b[39m.\u001b[39m_log_destroy_pending \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m     83\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mnot\u001b[39;00m f\u001b[39m.\u001b[39mdone():\n\u001b[0;32m---> 84\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_once()\n\u001b[1;32m     85\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_stopping:\n\u001b[1;32m     86\u001b[0m         \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/jj1.3/lib/python3.8/site-packages/nest_asyncio.py:107\u001b[0m, in \u001b[0;36m_patch_loop.<locals>._run_once\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    100\u001b[0m     heappop(scheduled)\n\u001b[1;32m    102\u001b[0m timeout \u001b[39m=\u001b[39m (\n\u001b[1;32m    103\u001b[0m     \u001b[39m0\u001b[39m \u001b[39mif\u001b[39;00m ready \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_stopping\n\u001b[1;32m    104\u001b[0m     \u001b[39melse\u001b[39;00m \u001b[39mmin\u001b[39m(\u001b[39mmax\u001b[39m(\n\u001b[1;32m    105\u001b[0m         scheduled[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39m_when \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtime(), \u001b[39m0\u001b[39m), \u001b[39m86400\u001b[39m) \u001b[39mif\u001b[39;00m scheduled\n\u001b[1;32m    106\u001b[0m     \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m)\n\u001b[0;32m--> 107\u001b[0m event_list \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_selector\u001b[39m.\u001b[39;49mselect(timeout)\n\u001b[1;32m    108\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_process_events(event_list)\n\u001b[1;32m    110\u001b[0m end_time \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtime() \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_clock_resolution\n",
      "File \u001b[0;32m~/anaconda3/envs/jj1.3/lib/python3.8/selectors.py:468\u001b[0m, in \u001b[0;36mEpollSelector.select\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    466\u001b[0m ready \u001b[39m=\u001b[39m []\n\u001b[1;32m    467\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 468\u001b[0m     fd_event_list \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_selector\u001b[39m.\u001b[39;49mpoll(timeout, max_ev)\n\u001b[1;32m    469\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mInterruptedError\u001b[39;00m:\n\u001b[1;32m    470\u001b[0m     \u001b[39mreturn\u001b[39;00m ready\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_federated as tff\n",
    "from tensorflow.keras import layers, models\n",
    "# from carbontracker.tracker import CarbonTracker\n",
    "# tracker = CarbonTracker(epochs=10)\n",
    "\n",
    "\n",
    "\n",
    "from pyJoules.energy_meter import measure_energy\n",
    "# @measure_energy\n",
    "\n",
    "\n",
    "\n",
    "\t# ...\n",
    "\t# Instructions to be evaluated.\n",
    "# print('helloworld')\n",
    "\t# ...\n",
    "\n",
    "\n",
    "\n",
    "# Load the CIFAR-10 dataset\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()\n",
    "\n",
    "# Preprocess the data\n",
    "def preprocess(x, y):\n",
    "    x = tf.cast(x, tf.float32) / 255.0\n",
    "    y = tf.cast(y, tf.int64)\n",
    "    return x, y\n",
    "\n",
    "x_train, y_train = preprocess(x_train, y_train)\n",
    "x_test, y_test = preprocess(x_test, y_test)\n",
    "\n",
    "# Create the Keras model\n",
    "@measure_energy\n",
    "def create_cifar10_model():\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3)))\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "    model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "    model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(64, activation='relu'))\n",
    "    model.add(layers.Dense(10))\n",
    "    return model\n",
    "\n",
    "# Create a TFF version of the Keras model \n",
    "def model_fn():\n",
    "    keras_model = create_cifar10_model() #(HERE U CHANGE THE MODEL)\n",
    "    return tff.learning.from_keras_model(\n",
    "        keras_model,\n",
    "        input_spec=(\n",
    "            tf.TensorSpec(shape=(None, 32, 32, 3), dtype=tf.float32),\n",
    "            tf.TensorSpec(shape=(None, 1), dtype=tf.int64),\n",
    "        ),\n",
    "        loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "        metrics=[tf.keras.metrics.SparseCategoricalAccuracy()],\n",
    "    )\n",
    "\n",
    "# Simulate federated data by splitting the dataset into multiple clients\n",
    "def split_data_for_clients(data, client_count):\n",
    "    client_data = []\n",
    "    data_len = len(data[0])\n",
    "    batch_size = data_len // client_count\n",
    "\n",
    "    for i in range(client_count):\n",
    "        start = i * batch_size\n",
    "        end = (i + 1) * batch_size if i != client_count - 1 else data_len\n",
    "        client_data.append((data[0][start:end], data[1][start:end]))\n",
    "\n",
    "    return client_data\n",
    "\n",
    "client_count = 10\n",
    "client_data = split_data_for_clients((x_train, y_train), client_count)\n",
    "\n",
    "# Create a federated dataset from the client data\n",
    "federated_data = [\n",
    "    tff.simulation.ClientData.from_clients_and_fn(\n",
    "        client_ids=[str(i)], create_tf_dataset_for_client_fn=lambda _: tf.data.Dataset.from_tensor_slices(client_data[i]).batch(20)\n",
    "    ).create_tf_dataset_for_client(str(i))\n",
    "    for i in range(client_count)\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# tracker.stop()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# Train the federated model\n",
    "trainer = tff.learning.build_federated_averaging_process(\n",
    "    model_fn,\n",
    "    client_optimizer_fn=lambda: tf.keras.optimizers.SGD(learning_rate=0.02),\n",
    "    server_optimizer_fn=lambda: tf.keras.optimizers.SGD(learning_rate=1.0),\n",
    ")\n",
    "\n",
    "# Initialize the state\n",
    "state = trainer.initialize()\n",
    "\n",
    "# Train the model for multiple rounds\n",
    "num_rounds = 10\n",
    "for round_num in range(1, num_rounds + 1):\n",
    "    print(f\"Round {round_num}\")\n",
    "    state, metrics = trainer.next(state, federated_data)\n",
    "    print(f\"Metrics: {metrics}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # Convert the test dataset into a list of datasets\n",
    "client_test_data = split_data_for_clients((x_test, y_test), client_count)\n",
    "\n",
    "# Create a federated dataset from the client test data\n",
    "federated_test_data = [\n",
    "    tf.data.Dataset.from_tensor_slices(client_test_data[i]).batch(len(client_test_data[i][0]))\n",
    "    for i in range(client_count)\n",
    "]\n",
    "\n",
    "# Evaluate the trained model on the federated test dataset\n",
    "tff_evaluator = tff.learning.build_federated_evaluation(model_fn)\n",
    "test_metrics = tff_evaluator(state.model, federated_test_data)\n",
    "print(f\"Test Metrics: {test_metrics}\")\n",
    "\n",
    "\n",
    "meter.end()\n",
    "meter.result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/gaia/anaconda3/envs/jj1.1/lib/python3.8/site-packages/tensorflow_federated/python/core/impl/compiler/tensorflow_computation_transformations.py:58: extract_sub_graph (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.compat.v1.graph_util.extract_sub_graph`\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/gaia/anaconda3/envs/jj1.1/lib/python3.8/site-packages/tensorflow_federated/python/core/impl/compiler/tensorflow_computation_transformations.py:58: extract_sub_graph (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.compat.v1.graph_util.extract_sub_graph`\n",
      "2023-04-25 16:04:32.020188: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2023-04-25 16:04:32.043210: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2803200000 Hz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Round 1\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 18\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m round_num \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, num_rounds \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRound \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mround_num\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 18\u001b[0m     state, metrics \u001b[38;5;241m=\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnext\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfederated_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMetrics: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmetrics\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/jj1.1/lib/python3.8/site-packages/tensorflow_federated/python/core/impl/computation/function_utils.py:526\u001b[0m, in \u001b[0;36mConcreteFunction.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    524\u001b[0m context \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_context_stack\u001b[39m.\u001b[39mcurrent\n\u001b[1;32m    525\u001b[0m arg \u001b[39m=\u001b[39m pack_args(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_type_signature\u001b[39m.\u001b[39mparameter, args, kwargs, context)\n\u001b[0;32m--> 526\u001b[0m \u001b[39mreturn\u001b[39;00m context\u001b[39m.\u001b[39;49minvoke(\u001b[39mself\u001b[39;49m, arg)\n",
      "File \u001b[0;32m~/anaconda3/envs/jj1.1/lib/python3.8/site-packages/retrying.py:56\u001b[0m, in \u001b[0;36mretry.<locals>.wrap.<locals>.wrapped_f\u001b[0;34m(*args, **kw)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[39m@six\u001b[39m\u001b[39m.\u001b[39mwraps(f)\n\u001b[1;32m     55\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapped_f\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkw):\n\u001b[0;32m---> 56\u001b[0m     \u001b[39mreturn\u001b[39;00m Retrying(\u001b[39m*\u001b[39;49mdargs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mdkw)\u001b[39m.\u001b[39;49mcall(f, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkw)\n",
      "File \u001b[0;32m~/anaconda3/envs/jj1.1/lib/python3.8/site-packages/retrying.py:257\u001b[0m, in \u001b[0;36mRetrying.call\u001b[0;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m    254\u001b[0m     attempt \u001b[39m=\u001b[39m Attempt(tb, attempt_number, \u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m    256\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mshould_reject(attempt):\n\u001b[0;32m--> 257\u001b[0m     \u001b[39mreturn\u001b[39;00m attempt\u001b[39m.\u001b[39;49mget(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_wrap_exception)\n\u001b[1;32m    259\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_after_attempts:\n\u001b[1;32m    260\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_after_attempts(attempt_number)\n",
      "File \u001b[0;32m~/anaconda3/envs/jj1.1/lib/python3.8/site-packages/retrying.py:301\u001b[0m, in \u001b[0;36mAttempt.get\u001b[0;34m(self, wrap_exception)\u001b[0m\n\u001b[1;32m    299\u001b[0m         \u001b[39mraise\u001b[39;00m RetryError(\u001b[39mself\u001b[39m)\n\u001b[1;32m    300\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 301\u001b[0m         six\u001b[39m.\u001b[39;49mreraise(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mvalue[\u001b[39m0\u001b[39;49m], \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mvalue[\u001b[39m1\u001b[39;49m], \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mvalue[\u001b[39m2\u001b[39;49m])\n\u001b[1;32m    302\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    303\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvalue\n",
      "File \u001b[0;32m~/anaconda3/envs/jj1.1/lib/python3.8/site-packages/six.py:719\u001b[0m, in \u001b[0;36mreraise\u001b[0;34m(tp, value, tb)\u001b[0m\n\u001b[1;32m    717\u001b[0m     \u001b[39mif\u001b[39;00m value\u001b[39m.\u001b[39m__traceback__ \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m tb:\n\u001b[1;32m    718\u001b[0m         \u001b[39mraise\u001b[39;00m value\u001b[39m.\u001b[39mwith_traceback(tb)\n\u001b[0;32m--> 719\u001b[0m     \u001b[39mraise\u001b[39;00m value\n\u001b[1;32m    720\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m    721\u001b[0m     value \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/jj1.1/lib/python3.8/site-packages/retrying.py:251\u001b[0m, in \u001b[0;36mRetrying.call\u001b[0;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m    248\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_before_attempts(attempt_number)\n\u001b[1;32m    250\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 251\u001b[0m     attempt \u001b[39m=\u001b[39m Attempt(fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs), attempt_number, \u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m    252\u001b[0m \u001b[39mexcept\u001b[39;00m:\n\u001b[1;32m    253\u001b[0m     tb \u001b[39m=\u001b[39m sys\u001b[39m.\u001b[39mexc_info()\n",
      "File \u001b[0;32m~/anaconda3/envs/jj1.1/lib/python3.8/site-packages/tensorflow_federated/python/core/impl/executors/execution_context.py:215\u001b[0m, in \u001b[0;36mExecutionContext.invoke\u001b[0;34m(self, comp, arg)\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[39mif\u001b[39;00m arg \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    211\u001b[0m   arg \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_event_loop\u001b[39m.\u001b[39mrun_until_complete(\n\u001b[1;32m    212\u001b[0m       tracing\u001b[39m.\u001b[39mwrap_coroutine_in_current_trace_context(\n\u001b[1;32m    213\u001b[0m           _ingest(executor, unwrapped_arg, arg\u001b[39m.\u001b[39mtype_signature)))\n\u001b[0;32m--> 215\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_event_loop\u001b[39m.\u001b[39;49mrun_until_complete(\n\u001b[1;32m    216\u001b[0m     tracing\u001b[39m.\u001b[39;49mwrap_coroutine_in_current_trace_context(\n\u001b[1;32m    217\u001b[0m         _invoke(executor, comp, arg, result_type)))\n",
      "File \u001b[0;32m~/anaconda3/envs/jj1.1/lib/python3.8/site-packages/nest_asyncio.py:84\u001b[0m, in \u001b[0;36m_patch_loop.<locals>.run_until_complete\u001b[0;34m(self, future)\u001b[0m\n\u001b[1;32m     82\u001b[0m     f\u001b[39m.\u001b[39m_log_destroy_pending \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m     83\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mnot\u001b[39;00m f\u001b[39m.\u001b[39mdone():\n\u001b[0;32m---> 84\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_once()\n\u001b[1;32m     85\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_stopping:\n\u001b[1;32m     86\u001b[0m         \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/jj1.1/lib/python3.8/site-packages/nest_asyncio.py:107\u001b[0m, in \u001b[0;36m_patch_loop.<locals>._run_once\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    100\u001b[0m     heappop(scheduled)\n\u001b[1;32m    102\u001b[0m timeout \u001b[39m=\u001b[39m (\n\u001b[1;32m    103\u001b[0m     \u001b[39m0\u001b[39m \u001b[39mif\u001b[39;00m ready \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_stopping\n\u001b[1;32m    104\u001b[0m     \u001b[39melse\u001b[39;00m \u001b[39mmin\u001b[39m(\u001b[39mmax\u001b[39m(\n\u001b[1;32m    105\u001b[0m         scheduled[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39m_when \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtime(), \u001b[39m0\u001b[39m), \u001b[39m86400\u001b[39m) \u001b[39mif\u001b[39;00m scheduled\n\u001b[1;32m    106\u001b[0m     \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m)\n\u001b[0;32m--> 107\u001b[0m event_list \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_selector\u001b[39m.\u001b[39;49mselect(timeout)\n\u001b[1;32m    108\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_process_events(event_list)\n\u001b[1;32m    110\u001b[0m end_time \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtime() \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_clock_resolution\n",
      "File \u001b[0;32m~/anaconda3/envs/jj1.1/lib/python3.8/selectors.py:468\u001b[0m, in \u001b[0;36mEpollSelector.select\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    466\u001b[0m ready \u001b[39m=\u001b[39m []\n\u001b[1;32m    467\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 468\u001b[0m     fd_event_list \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_selector\u001b[39m.\u001b[39;49mpoll(timeout, max_ev)\n\u001b[1;32m    469\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mInterruptedError\u001b[39;00m:\n\u001b[1;32m    470\u001b[0m     \u001b[39mreturn\u001b[39;00m ready\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# Train the federated model\n",
    "trainer = tff.learning.build_federated_averaging_process(\n",
    "    model_fn,\n",
    "    client_optimizer_fn=lambda: tf.keras.optimizers.SGD(learning_rate=0.02),\n",
    "    server_optimizer_fn=lambda: tf.keras.optimizers.SGD(learning_rate=1.0),\n",
    ")\n",
    "\n",
    "# Initialize the state\n",
    "state = trainer.initialize()\n",
    "\n",
    "# Train the model for multiple rounds\n",
    "num_rounds = 10\n",
    "for round_num in range(1, num_rounds + 1):\n",
    "    print(f\"Round {round_num}\")\n",
    "    state, metrics = trainer.next(state, federated_data)\n",
    "    print(f\"Metrics: {metrics}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the test dataset into a list of datasets\n",
    "client_test_data = split_data_for_clients((x_test, y_test), client_count)\n",
    "\n",
    "# Create a federated dataset from the client test data\n",
    "federated_test_data = [\n",
    "    tf.data.Dataset.from_tensor_slices(client_test_data[i]).batch(len(client_test_data[i][0]))\n",
    "    for i in range(client_count)\n",
    "]\n",
    "\n",
    "# Evaluate the trained model on the federated test dataset\n",
    "tff_evaluator = tff.learning.build_federated_evaluation(model_fn)\n",
    "test_metrics = tff_evaluator(state.model, federated_test_data)\n",
    "print(f\"Test Metrics: {test_metrics}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-01 08:27:15.117356: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2023-05-01 08:27:15.118106: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2023-05-01 08:27:15.118117: W tensorflow/stream_executor/cuda/cuda_driver.cc:326] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2023-05-01 08:27:15.118132: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (gaia-XPS-13-9310): /proc/driver/nvidia/version does not exist\n",
      "2023-05-01 08:27:15.118683: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-05-01 08:27:15.119385: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2023-05-01 08:27:15.617420: W tensorflow/core/framework/cpu_allocator_impl.cc:80] Allocation of 614400000 exceeds 10% of free system memory.\n",
      "2023-05-01 08:27:15.692923: W tensorflow/core/framework/cpu_allocator_impl.cc:80] Allocation of 614400000 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/gaia/anaconda3/envs/jj1.3/lib/python3.8/site-packages/tensorflow_federated/python/core/impl/compiler/tensorflow_computation_transformations.py:58: extract_sub_graph (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.compat.v1.graph_util.extract_sub_graph`\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/gaia/anaconda3/envs/jj1.3/lib/python3.8/site-packages/tensorflow_federated/python/core/impl/compiler/tensorflow_computation_transformations.py:58: extract_sub_graph (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.compat.v1.graph_util.extract_sub_graph`\n",
      "2023-05-01 08:27:19.381655: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2023-05-01 08:27:19.413478: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2803200000 Hz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Round 1\n",
      "Metrics: OrderedDict([('broadcast', ()), ('aggregation', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('train', OrderedDict([('sparse_categorical_accuracy', 0.15592), ('loss', 2.2329133)])), ('stat', OrderedDict([('num_examples', 50000)]))])\n",
      "Metrics: OrderedDict([('broadcast', ()), ('aggregation', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('train', OrderedDict([('sparse_categorical_accuracy', 0.23264), ('loss', 2.0789487)])), ('stat', OrderedDict([('num_examples', 25000)]))])\n",
      "Round 2\n",
      "Metrics: OrderedDict([('broadcast', ()), ('aggregation', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('train', OrderedDict([('sparse_categorical_accuracy', 0.26964), ('loss', 2.0018804)])), ('stat', OrderedDict([('num_examples', 50000)]))])\n",
      "Metrics: OrderedDict([('broadcast', ()), ('aggregation', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('train', OrderedDict([('sparse_categorical_accuracy', 0.30556), ('loss', 1.923649)])), ('stat', OrderedDict([('num_examples', 25000)]))])\n",
      "Round 3\n",
      "Metrics: OrderedDict([('broadcast', ()), ('aggregation', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('train', OrderedDict([('sparse_categorical_accuracy', 0.34112), ('loss', 1.8317631)])), ('stat', OrderedDict([('num_examples', 50000)]))])\n",
      "Metrics: OrderedDict([('broadcast', ()), ('aggregation', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('train', OrderedDict([('sparse_categorical_accuracy', 0.36672), ('loss', 1.7565339)])), ('stat', OrderedDict([('num_examples', 25000)]))])\n",
      "Round 4\n",
      "Metrics: OrderedDict([('broadcast', ()), ('aggregation', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('train', OrderedDict([('sparse_categorical_accuracy', 0.39414), ('loss', 1.6824145)])), ('stat', OrderedDict([('num_examples', 50000)]))])\n",
      "Metrics: OrderedDict([('broadcast', ()), ('aggregation', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('train', OrderedDict([('sparse_categorical_accuracy', 0.41048), ('loss', 1.6238614)])), ('stat', OrderedDict([('num_examples', 25000)]))])\n",
      "Round 5\n",
      "Metrics: OrderedDict([('broadcast', ()), ('aggregation', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('train', OrderedDict([('sparse_categorical_accuracy', 0.43162), ('loss', 1.572088)])), ('stat', OrderedDict([('num_examples', 50000)]))])\n",
      "Metrics: OrderedDict([('broadcast', ()), ('aggregation', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('train', OrderedDict([('sparse_categorical_accuracy', 0.44348), ('loss', 1.5315063)])), ('stat', OrderedDict([('num_examples', 25000)]))])\n",
      "Round 6\n",
      "Metrics: OrderedDict([('broadcast', ()), ('aggregation', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('train', OrderedDict([('sparse_categorical_accuracy', 0.46274), ('loss', 1.4912425)])), ('stat', OrderedDict([('num_examples', 50000)]))])\n",
      "Metrics: OrderedDict([('broadcast', ()), ('aggregation', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('train', OrderedDict([('sparse_categorical_accuracy', 0.47208), ('loss', 1.4589759)])), ('stat', OrderedDict([('num_examples', 25000)]))])\n",
      "Round 7\n",
      "Metrics: OrderedDict([('broadcast', ()), ('aggregation', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('train', OrderedDict([('sparse_categorical_accuracy', 0.48866), ('loss', 1.4275095)])), ('stat', OrderedDict([('num_examples', 50000)]))])\n",
      "Metrics: OrderedDict([('broadcast', ()), ('aggregation', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('train', OrderedDict([('sparse_categorical_accuracy', 0.50048), ('loss', 1.3936332)])), ('stat', OrderedDict([('num_examples', 25000)]))])\n",
      "Round 8\n",
      "Metrics: OrderedDict([('broadcast', ()), ('aggregation', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('train', OrderedDict([('sparse_categorical_accuracy', 0.5096), ('loss', 1.3730086)])), ('stat', OrderedDict([('num_examples', 50000)]))])\n",
      "Metrics: OrderedDict([('broadcast', ()), ('aggregation', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('train', OrderedDict([('sparse_categorical_accuracy', 0.519), ('loss', 1.3525256)])), ('stat', OrderedDict([('num_examples', 25000)]))])\n",
      "Round 9\n",
      "Metrics: OrderedDict([('broadcast', ()), ('aggregation', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('train', OrderedDict([('sparse_categorical_accuracy', 0.52862), ('loss', 1.3237197)])), ('stat', OrderedDict([('num_examples', 50000)]))])\n",
      "Metrics: OrderedDict([('broadcast', ()), ('aggregation', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('train', OrderedDict([('sparse_categorical_accuracy', 0.5394), ('loss', 1.2927827)])), ('stat', OrderedDict([('num_examples', 25000)]))])\n",
      "Round 10\n",
      "Metrics: OrderedDict([('broadcast', ()), ('aggregation', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('train', OrderedDict([('sparse_categorical_accuracy', 0.54528), ('loss', 1.2784895)])), ('stat', OrderedDict([('num_examples', 50000)]))])\n",
      "Metrics: OrderedDict([('broadcast', ()), ('aggregation', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('train', OrderedDict([('sparse_categorical_accuracy', 0.5564), ('loss', 1.2554361)])), ('stat', OrderedDict([('num_examples', 25000)]))])\n",
      "Test Metrics: OrderedDict([('sparse_categorical_accuracy', 0.5761), ('loss', 1.1962435)])\n",
      "begin timestamp : 1682922434.6066718; tag : fed_model; duration : 359.23001074790955; package_0 : 3355976577.0; core_0 : 2392279825.0; uncore_0 : 290833118.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "OrderedDict([('sparse_categorical_accuracy', 0.5761), ('loss', 1.1962435)])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_federated as tff\n",
    "from tensorflow.keras import layers, models\n",
    "import nest_asyncio\n",
    "from tensorflow.keras.applications.vgg16 import VGG16\n",
    "from tensorflow.keras.layers import Input, Flatten, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "from pyJoules.energy_meter import measure_energy\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "@measure_energy\n",
    "def fed_model(num_rounds, clients_per_round ):\n",
    "\n",
    "    # Load the CIFAR-10 dataset\n",
    "    (x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()\n",
    "\n",
    "    # Preprocess the data\n",
    "    def preprocess(x, y):\n",
    "        x = tf.cast(x, tf.float32) / 255.0\n",
    "        y = tf.cast(y, tf.int64)\n",
    "        return x, y\n",
    "\n",
    "\n",
    "    x_train, y_train = preprocess(x_train, y_train)\n",
    "    x_test, y_test = preprocess(x_test, y_test)\n",
    "\n",
    "    # # Create the Keras model\n",
    "    # def create_cifar10_model():\n",
    "    #     model = models.Sequential()\n",
    "    #     model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3)))\n",
    "    #     model.add(layers.MaxPooling2D((2, 2)))\n",
    "    #     model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "    #     model.add(layers.MaxPooling2D((2, 2)))\n",
    "    #     model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "    #     model.add(layers.Flatten())\n",
    "    #     model.add(layers.Dense(64, activation='relu'))\n",
    "    #     model.add(layers.Dense(10))\n",
    "    #     return model\n",
    "    \n",
    "    def create_vgg16_model():\n",
    "        # Load the VGG-16 model\n",
    "        base_model = VGG16(weights='imagenet', include_top=False, input_tensor=Input(shape=(32, 32, 3)))\n",
    "    \n",
    "        # Add custom layers on top of the VGG-16 model\n",
    "        x = Flatten()(base_model.output)\n",
    "        x = Dense(256, activation='relu')(x)\n",
    "        x = Dense(128, activation='relu')(x)\n",
    "        predictions = Dense(10)(x)\n",
    "\n",
    "        # Create the model\n",
    "        model = Model(inputs=base_model.input, outputs=predictions)\n",
    "        return model\n",
    "\n",
    "    # # Create a TFF version of the Keras model \n",
    "    # def model_fn():\n",
    "    #     keras_model = create_cifar10_model() #(HERE U CHANGE THE MODEL)\n",
    "    #     return tff.learning.from_keras_model(\n",
    "    #         keras_model,\n",
    "    #         input_spec=(\n",
    "    #             tf.TensorSpec(shape=(None, 32, 32, 3), dtype=tf.float32),\n",
    "    #             tf.TensorSpec(shape=(None, 1), dtype=tf.int64),\n",
    "    #         ),\n",
    "    #         loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    #         metrics=[tf.keras.metrics.SparseCategoricalAccuracy()],\n",
    "    #     )\n",
    "\n",
    "\n",
    "\n",
    "    # Create a TFF version of the Keras model \n",
    "    def model_fn():\n",
    "        keras_model = create_vgg16_model()\n",
    "        return tff.learning.from_keras_model(\n",
    "            keras_model,\n",
    "            input_spec=(\n",
    "                tf.TensorSpec(shape=(None, 32, 32, 3), dtype=tf.float32),\n",
    "                tf.TensorSpec(shape=(None, 1), dtype=tf.int64),\n",
    "            ),\n",
    "            loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "            metrics=[tf.keras.metrics.SparseCategoricalAccuracy()],\n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # Simulate federated data by splitting the dataset into multiple clients\n",
    "    def split_data_for_clients(data, client_count):\n",
    "        client_data = []\n",
    "        data_len = len(data[0])\n",
    "        batch_size = data_len // client_count\n",
    "\n",
    "        for i in range(client_count):\n",
    "            start = i * batch_size\n",
    "            end = (i + 1) * batch_size if i != client_count - 1 else data_len\n",
    "            client_data.append((data[0][start:end], data[1][start:end]))\n",
    "\n",
    "        return client_data\n",
    "\n",
    "    client_count = 10\n",
    "    client_data = split_data_for_clients((x_train, y_train), client_count)\n",
    "\n",
    "    # Create a federated dataset from the client data\n",
    "    federated_data = [\n",
    "        tff.simulation.ClientData.from_clients_and_fn(\n",
    "            client_ids=[str(i)], create_tf_dataset_for_client_fn=lambda _: tf.data.Dataset.from_tensor_slices(client_data[i]).batch(20)\n",
    "        ).create_tf_dataset_for_client(str(i))\n",
    "        for i in range(client_count)\n",
    "    ]\n",
    "\n",
    "\n",
    "    nest_asyncio.apply()\n",
    "\n",
    "    # Train the federated model\n",
    "    trainer = tff.learning.build_federated_averaging_process(\n",
    "        model_fn,\n",
    "        client_optimizer_fn=lambda: tf.keras.optimizers.SGD(learning_rate=0.02),\n",
    "        server_optimizer_fn=lambda: tf.keras.optimizers.SGD(learning_rate=1.0),\n",
    "        # client_optimizer_fn=client_optimizer,\n",
    "        # server_optimizer_fn=server_optimizer,\n",
    "        client_weighting=tff.learning.ClientWeighting.NUM_EXAMPLES\n",
    "    )\n",
    "\n",
    "    # Initialize the state\n",
    "    state = trainer.initialize()\n",
    "\n",
    "    # Train the model for multiple rounds\n",
    "    # num_rounds = 10\n",
    "    for round_num in range(1, num_rounds + 1):\n",
    "        print(f\"Round {round_num}\")\n",
    "        state, metrics = trainer.next(state, federated_data) #?????????????????????????????????????????????????????????????????????????????????????????????????????\n",
    "        print(f\"Metrics: {metrics}\") #????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????\n",
    "        # Select a random subset of clients to participate in this round\n",
    "        client_ids = np.random.choice(client_count, clients_per_round, replace=False)\n",
    "        client_data_for_round = [federated_data[i] for i in client_ids]\n",
    "\n",
    "        # Train the model on the selected clients' data\n",
    "        state, metrics = trainer.next(state, client_data_for_round)\n",
    "        print(f\"Metrics: {metrics}\")\n",
    "\n",
    "\n",
    "\n",
    "    # Convert the test dataset into a list of datasets\n",
    "    client_test_data = split_data_for_clients((x_test, y_test), client_count)\n",
    "\n",
    "    # Create a federated dataset from the client test data\n",
    "    federated_test_data = [\n",
    "        tf.data.Dataset.from_tensor_slices(client_test_data[i]).batch(len(client_test_data[i][0]))\n",
    "        for i in range(client_count)\n",
    "    ]\n",
    "\n",
    "    # Evaluate the trained model on the federated test dataset\n",
    "    tff_evaluator = tff.learning.build_federated_evaluation(model_fn)\n",
    "    test_metrics = tff_evaluator(state.model, federated_test_data)\n",
    "    print(f\"Test Metrics: {test_metrics}\")\n",
    "    return test_metrics\n",
    "\n",
    "vgg = fed_model(10, 5)\n",
    "vgg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gaia/anaconda3/envs/jj1.3/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "WARNING:root:pynvml not found you can't use NVIDIA devices\n",
      "2023-05-04 00:30:34.926330: W tensorflow/core/framework/cpu_allocator_impl.cc:80] Allocation of 9830400000 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_federated as tff\n",
    "import nest_asyncio\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from timm.models import create_model\n",
    "from pyJoules.energy_meter import measure_energy\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "@measure_energy\n",
    "def fed_model(num_rounds, clients_per_round ):\n",
    "\n",
    "    # Load the CIFAR-10 dataset\n",
    "    (x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()\n",
    "\n",
    "    # Preprocess the data\n",
    "    def preprocess(x, y):\n",
    "        x = tf.cast(x, tf.float32) / 255.0\n",
    "        x = tf.image.resize(x, (128, 128))\n",
    "        y = tf.cast(y, tf.int64)\n",
    "        return x, y\n",
    "\n",
    "\n",
    "    x_train, y_train = preprocess(x_train, y_train)\n",
    "    x_test, y_test = preprocess(x_test, y_test)\n",
    "\n",
    "    # # Create the Keras model\n",
    "    # def create_cifar10_model():\n",
    "    #     model = models.Sequential()\n",
    "    #     model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3)))\n",
    "    #     model.add(layers.MaxPooling2D((2, 2)))\n",
    "    #     model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "    #     model.add(layers.MaxPooling2D((2, 2)))\n",
    "    #     model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "    #     model.add(layers.Flatten())\n",
    "    #     model.add(layers.Dense(64, activation='relu'))\n",
    "    #     model.add(layers.Dense(10))\n",
    "    #     return model\n",
    "    \n",
    "    # def create_vgg16_model():\n",
    "    #     # Load the VGG-16 model\n",
    "    #     base_model = VGG16(weights='imagenet', include_top=False, input_tensor=Input(shape=(32, 32, 3)))\n",
    "    \n",
    "    #     # Add custom layers on top of the VGG-16 model\n",
    "    #     x = Flatten()(base_model.output)\n",
    "    #     x = Dense(256, activation='relu')(x)\n",
    "    #     x = Dense(128, activation='relu')(x)\n",
    "    #     predictions = Dense(10)(x)\n",
    "\n",
    "    #     # Create the model\n",
    "    #     model = Model(inputs=base_model.input, outputs=predictions)\n",
    "    #     return model\n",
    "\n",
    "    # # Create a TFF version of the Keras model \n",
    "    # def model_fn():\n",
    "    #     keras_model = create_cifar10_model() #(HERE U CHANGE THE MODEL)\n",
    "    #     return tff.learning.from_keras_model(\n",
    "    #         keras_model,\n",
    "    #         input_spec=(\n",
    "    #             tf.TensorSpec(shape=(None, 32, 32, 3), dtype=tf.float32),\n",
    "    #             tf.TensorSpec(shape=(None, 1), dtype=tf.int64),\n",
    "    #         ),\n",
    "    #         loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    #         metrics=[tf.keras.metrics.SparseCategoricalAccuracy()],\n",
    "    #     )\n",
    "\n",
    "\n",
    "    # Convert the dataset to PyTorch tensors\n",
    "    x_train_pt = torch.from_numpy(np.transpose(x_train, (0, 3, 1, 2)))\n",
    "    y_train_pt = torch.from_numpy(y_train.squeeze())\n",
    "    x_test_pt = torch.from_numpy(np.transpose(x_test, (0, 3, 1, 2)))\n",
    "    y_test_pt = torch.from_numpy(y_test.squeeze())\n",
    "\n",
    "    # Create the ViT model using timm\n",
    "    def create_vit_model():\n",
    "        model = create_model('vit_tiny_patch16_224', pretrained=True)\n",
    "        model.head = torch.nn.Linear(model.head.in_features, 10)\n",
    "        return model\n",
    "\n",
    "    # Convert the ViT model to TensorFlow format\n",
    "    def convert_to_tf_model(vit_model):\n",
    "        input_shape = (None, 128, 128, 3)\n",
    "        img = tf.keras.Input(shape=input_shape[1:], dtype=tf.float32)\n",
    "        x = tf.keras.applications.vgg19.preprocess_input(img)\n",
    "        x = tf.image.resize(x, (224, 224))\n",
    "        outputs = vit_model(x)\n",
    "        outputs = tf.squeeze(outputs[:, 0, :])\n",
    "        return tf.keras.Model(inputs=img, outputs=outputs)\n",
    "\n",
    "\n",
    "\n",
    "    # Create a TFF version of the Keras model \n",
    "    def model_fn():\n",
    "        vit_model = create_vit_model()\n",
    "        tf_model = convert_to_tf_model(vit_model)\n",
    "        return tff.learning.from_keras_model(\n",
    "            tf_model,\n",
    "            input_spec=(\n",
    "                tf.TensorSpec(shape=(None, 224, 224, 3), dtype=tf.float32),\n",
    "                tf.TensorSpec(shape=(None, 1), dtype=tf.int64),\n",
    "        ),\n",
    "        loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "        metrics=[tf.keras.metrics.SparseCategoricalAccuracy()],\n",
    "    )\n",
    "\n",
    "\n",
    "def split_data_for_clients(data, client_count, batch_size):\n",
    "    client_data = []\n",
    "    data_len = len(data[0])\n",
    "    samples_per_client = data_len // client_count\n",
    "\n",
    "    for i in range(client_count):\n",
    "        start = i * samples_per_client\n",
    "        end = (i + 1) * samples_per_client\n",
    "        x = data[0][start:end]\n",
    "        y = data[1][start:end]\n",
    "        client_data.append(tf.data.Dataset.from_tensor_slices((x, y)).batch(batch_size))\n",
    "\n",
    "    return client_data\n",
    "    client_count = 10\n",
    "\n",
    "    client_data = split_data_for_clients((x_train, y_train), client_count, batch_size=10)\n",
    "\n",
    "\n",
    "\n",
    "    # # Simulate federated data by splitting the dataset into multiple clients\n",
    "    # def split_data_for_clients(data, client_count):\n",
    "    #     client_data = []\n",
    "    #     data_len = len(data[0])\n",
    "    #     batch_size = data_len // client_count\n",
    "\n",
    "    #     for i in range(client_count):\n",
    "    #         start = i * batch_size\n",
    "    #         end = (i + 1) * batch_size if i != client_count - 1 else data_len\n",
    "    #         client_data.append((data[0][start:end], data[1][start:end]))\n",
    "\n",
    "    #     return client_data\n",
    "\n",
    "    # client_count = 10\n",
    "    # client_data = split_data_for_clients((x_train_pt, y_train_pt), client_count)\n",
    "\n",
    "    # Create a federated dataset from the client data\n",
    "    federated_data = [\n",
    "        tff.simulation.ClientData.from_clients_and_fn(\n",
    "            client_ids=[str(i)], create_tf_dataset_for_client_fn=lambda _: tf.data.Dataset.from_tensor_slices(client_data[i]).batch(20)\n",
    "        ).create_tf_dataset_for_client(str(i))\n",
    "        for i in range(client_count)\n",
    "    ]\n",
    "\n",
    "\n",
    "    nest_asyncio.apply()\n",
    "\n",
    "    # Train the federated model\n",
    "    trainer = tff.learning.build_federated_averaging_process(\n",
    "        model_fn,\n",
    "        client_optimizer_fn=lambda: tf.keras.optimizers.SGD(learning_rate=0.02),\n",
    "        server_optimizer_fn=lambda: tf.keras.optimizers.SGD(learning_rate=1.0),\n",
    "        # client_optimizer_fn=client_optimizer,\n",
    "        # server_optimizer_fn=server_optimizer,\n",
    "        client_weighting=tff.learning.ClientWeighting.NUM_EXAMPLES\n",
    "    )\n",
    "\n",
    "    # Initialize the state\n",
    "    state = trainer.initialize()\n",
    "\n",
    "    # Train the model for multiple rounds\n",
    "    # num_rounds = 10\n",
    "    for round_num in range(1, num_rounds + 1):\n",
    "        print(f\"Round {round_num}\")\n",
    "        state, metrics = trainer.next(state, federated_data) #?????????????????????????????????????????????????????????????????????????????????????????????????????\n",
    "        print(f\"Metrics: {metrics}\") #????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????\n",
    "        # Select a random subset of clients to participate in this round\n",
    "        client_ids = np.random.choice(client_count, clients_per_round, replace=False)\n",
    "        client_data_for_round = [federated_data[i] for i in client_ids]\n",
    "\n",
    "        # Train the model on the selected clients' data\n",
    "        state, metrics = trainer.next(state, client_data_for_round)\n",
    "        print(f\"Metrics: {metrics}\")\n",
    "\n",
    "\n",
    "\n",
    "    # Convert the test dataset into a list of datasets\n",
    "    client_test_data = split_data_for_clients((x_test_pt, y_test_pt), client_count)\n",
    "\n",
    "    # Create a federated dataset from the client test data\n",
    "    federated_test_data = [\n",
    "        tf.data.Dataset.from_tensor_slices(client_test_data[i]).batch(len(client_test_data[i][0]))\n",
    "        for i in range(client_count)\n",
    "    ]\n",
    "\n",
    "    # Evaluate the trained model on the federated test dataset\n",
    "    tff_evaluator = tff.learning.build_federated_evaluation(model_fn)\n",
    "    test_metrics = tff_evaluator(state.model, federated_test_data)\n",
    "    print(f\"Test Metrics: {test_metrics}\")\n",
    "    return test_metrics\n",
    "\n",
    "vit = fed_model(10, 5)\n",
    "vit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "# num_epochs = 10\n",
    "\n",
    "\n",
    "# tf.keras.mixed_precision.set_global_policy('mixed_float16')\n",
    "# optimizer = tf.keras.optimizers.Adam()\n",
    "# loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "# train_acc_metric = tf.keras.metrics.SparseCategoricalAccuracy()\n",
    "# val_acc_metric = tf.keras.metrics.SparseCategoricalAccuracy()\n",
    "\n",
    "# @tf.function\n",
    "# def train_step(model, inputs, labels):\n",
    "#     with tf.GradientTape() as tape:\n",
    "#         logits = model(inputs, training=True)\n",
    "#         loss = loss_fn(labels, logits)\n",
    "#         scaled_loss = optimizer.get_scaled_loss(loss)\n",
    "#     scaled_gradients = tape.gradient(scaled_loss, model.trainable_variables)\n",
    "#     gradients = optimizer.get_unscaled_gradients(scaled_gradients)\n",
    "#     optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "#     train_acc_metric.update_state(labels, logits)\n",
    "\n",
    "# @tf.function\n",
    "# def val_step(model, inputs, labels):\n",
    "#     logits = model(inputs, training=False)\n",
    "#     val_acc_metric.update_state(labels, logits)\n",
    "\n",
    "# for epoch in range(num_epochs):\n",
    "#     train_acc_metric.reset_states()\n",
    "#     for batch in train_data:\n",
    "#         train_step(model, *batch)\n",
    "#     train_acc = train_acc_metric.result()\n",
    "#     val_acc_metric.reset_states()\n",
    "#     for batch in val_data:\n",
    "#         val_step(model, *batch)\n",
    "#     val_acc = val_acc_metric.result()\n",
    "#     print(f\"Epoch {epoch + 1}: Train Acc = {train_acc:.4f}, Val Acc = {val_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.16 ('jj1.3')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "930989fc6d0490d3510446dd72a4f581e5cccf0cebb3935cb86039c2b3c0a685"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
